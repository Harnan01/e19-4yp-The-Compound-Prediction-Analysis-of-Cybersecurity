{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBwQb7WrIDi1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d677c949-1da3-4e43-deb1-f4676f224210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.11/dist-packages (2025.5.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (3.1.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (2025.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (25.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (1.0.0)\n",
            "Requirement already satisfied: importlib_metadata>=4.13.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (8.7.0)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (20.0.0)\n",
            "Requirement already satisfied: lz4>=4.3.2 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (4.4.4)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.13.0->dask[complete]) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.11/dist-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (2.2.2)\n",
            "Requirement already satisfied: distributed==2025.5.1 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (2025.5.1)\n",
            "Requirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (3.7.3)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.11/dist-packages (from dask[complete]) (3.1.6)\n",
            "Requirement already satisfied: msgpack>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.1->dask[complete]) (1.1.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.1->dask[complete]) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.1->dask[complete]) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.1->dask[complete]) (3.1.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.1->dask[complete]) (6.4.2)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.1->dask[complete]) (2.4.0)\n",
            "Requirement already satisfied: zict>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from distributed==2025.5.1->dask[complete]) (3.0.0)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0->dask[complete]) (1.3.2)\n",
            "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0->dask[complete]) (1.40.0)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0->dask[complete]) (11.2.1)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh>=3.1.0->dask[complete]) (2025.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.10.3->dask[complete]) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[complete]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0->dask[complete]) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install dask[complete] scikit-learn joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import all required libraries\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.bag as db\n",
        "import dask.dataframe as dd\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import gzip\n",
        "from glob import glob\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All dependencies installed and imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWlzzK1YwvOy",
        "outputId": "ce0e9fd8-189b-4732-a3dd-facdf90965b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All dependencies installed and imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_logging():\n",
        "    \"\"\"Setup detailed logging for tracking pipeline execution\"\"\"\n",
        "    log_format = '%(asctime)s - %(levelname)s - %(funcName)s - %(message)s'\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        format=log_format,\n",
        "        handlers=[\n",
        "            logging.FileHandler('/content/vulnerability_detection.log'),\n",
        "            logging.StreamHandler()\n",
        "        ]\n",
        "    )\n",
        "    return logging.getLogger(__name__)\n",
        "\n",
        "# Initialize logger\n",
        "logger = setup_logging()\n",
        "logger.info(\"üîß Logging system initialized\")"
      ],
      "metadata": {
        "id": "8rkCdzAExCrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive():\n",
        "    \"\"\"Mount Google Drive and verify access\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        logger.info(\"‚úÖ Google Drive mounted successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Failed to mount Google Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "# Mount Google Drive\n",
        "if mount_drive():\n",
        "    print(\"‚úÖ Google Drive is ready!\")\n",
        "else:\n",
        "    print(\"‚ùå Please check your Google Drive connection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmflitWFxHYk",
        "outputId": "9a173d1c-2798-457c-8491-7624b4db95ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_drive():\n",
        "    \"\"\"Mount Google Drive and verify access\"\"\"\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        logger.info(\"‚úÖ Google Drive mounted successfully\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Failed to mount Google Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "# Mount Google Drive\n",
        "if mount_drive():\n",
        "    print(\"‚úÖ Google Drive is ready!\")\n",
        "else:\n",
        "    print(\"‚ùå Please check your Google Drive connection\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq4RPl2pxU_5",
        "outputId": "37393ca6-5b47-40d2-d0cb-4d11a93f78d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_directory(log_dir):\n",
        "    \"\"\"Verify log directory exists and contains files\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(log_dir):\n",
        "            logger.error(f\"‚ùå Directory not found: {log_dir}\")\n",
        "            return False\n",
        "\n",
        "        files = os.listdir(log_dir)\n",
        "        log_files = [f for f in files if f.endswith(('.log', '.gz', '.txt'))]\n",
        "\n",
        "        logger.info(f\"üìÅ Directory: {log_dir}\")\n",
        "        logger.info(f\"üìä Total files: {len(files)}, Log files: {len(log_files)}\")\n",
        "\n",
        "        if len(log_files) == 0:\n",
        "            logger.warning(\"‚ö†Ô∏è No log files found in directory\")\n",
        "            return False\n",
        "\n",
        "        # Sample first few filenames\n",
        "        sample_files = log_files[:5]\n",
        "        logger.info(f\"üìÑ Sample files: {sample_files}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error verifying directory: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_log_files(log_dir):\n",
        "    \"\"\"Get and filter log files from directory\"\"\"\n",
        "    try:\n",
        "        all_files = os.listdir(log_dir)\n",
        "        log_files = [f for f in all_files if f.endswith(('.log', '.gz', '.txt'))]\n",
        "        full_paths = [os.path.join(log_dir, f) for f in log_files]\n",
        "\n",
        "        logger.info(f\"üîç Found {len(log_files)} log files for processing\")\n",
        "\n",
        "        # Check file sizes for processing estimation\n",
        "        total_size = 0\n",
        "        for path in full_paths[:10]:  # Sample first 10 files\n",
        "            try:\n",
        "                size = os.path.getsize(path)\n",
        "                total_size += size\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        avg_size = total_size / min(10, len(full_paths)) if full_paths else 0\n",
        "        estimated_total = (avg_size * len(full_paths)) / (1024*1024)  # MB\n",
        "        logger.info(f\"üìè Estimated total data size: {estimated_total:.2f} MB\")\n",
        "\n",
        "        return full_paths\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error getting log files: {e}\")\n",
        "        return []\n",
        "\n",
        "# Verify your log directory (update the path as needed)\n",
        "log_dir = '/content/drive/MyDrive/self_logs'\n",
        "if verify_directory(log_dir):\n",
        "    file_paths = get_log_files(log_dir)\n",
        "    print(f\"‚úÖ Found {len(file_paths)} log files to process\")\n",
        "else:\n",
        "    print(\"‚ùå Please check your log directory path\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVuYXwPWxbWm",
        "outputId": "386a39ba-2eba-45a9-82c2-4ebe024d1de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Found 2741 log files to process\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_log_line(line):\n",
        "    \"\"\"Parse Apache Combined Log Format with vulnerability focus\"\"\"\n",
        "    # Enhanced regex for Apache Combined Log Format\n",
        "    patterns = [\n",
        "        # Standard Combined Log Format\n",
        "        r'(\\S+) \\S+ \\S+ \\[(.*?)\\] \"(\\S+) (.*?) (\\S+)\" (\\d+) (\\d+|-) \"(.*?)\" \"(.*?)\"',\n",
        "        # Common Log Format fallback\n",
        "        r'(\\S+) \\S+ \\S+ \\[(.*?)\\] \"(\\S+) (.*?) (\\S+)\" (\\d+) (\\d+|-)',\n",
        "        # Simple format fallback\n",
        "        r'(\\S+).*?\\[(.*?)\\].*?\"(\\S+) (.*?) (\\S+)\" (\\d+)'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.match(pattern, line.strip())\n",
        "        if match:\n",
        "            groups = match.groups()\n",
        "            # Pad with empty strings if needed\n",
        "            while len(groups) < 9:\n",
        "                groups = groups + ('',)\n",
        "            return groups[:9]  # Ensure exactly 9 fields\n",
        "    return None\n",
        "\n",
        "def process_log_file(file_path):\n",
        "    \"\"\"Process individual log file with error handling\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üìñ Processing: {os.path.basename(file_path)}\")\n",
        "\n",
        "        # Handle different file types\n",
        "        if file_path.endswith('.gz'):\n",
        "            with gzip.open(file_path, 'rt', encoding='utf-8', errors='ignore') as f:\n",
        "                lines = f.readlines()\n",
        "        else:\n",
        "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                lines = f.readlines()\n",
        "\n",
        "        logger.info(f\"üìù File contains {len(lines)} lines\")\n",
        "\n",
        "        # Parse lines using Dask for efficiency\n",
        "        parsed_lines = []\n",
        "        for line in lines:\n",
        "            parsed = parse_log_line(line)\n",
        "            if parsed:\n",
        "                parsed_lines.append(parsed)\n",
        "\n",
        "        success_rate = len(parsed_lines) / len(lines) * 100 if lines else 0\n",
        "        logger.info(f\"‚úÖ Parsed {len(parsed_lines)}/{len(lines)} lines ({success_rate:.1f}% success)\")\n",
        "\n",
        "        return db.from_sequence(parsed_lines)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error processing {file_path}: {e}\")\n",
        "        return db.from_sequence([])\n",
        "\n",
        "print(\"‚úÖ Log parsing functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NsMWu-Axh5C",
        "outputId": "f3f1eef9-319e-4374-e005-d15d958df76a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Log parsing functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_all_logs(file_paths):\n",
        "    \"\"\"Parse all log files and combine into DataFrame\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"üöÄ Starting to parse {len(file_paths)} log files...\")\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Process files in batches to manage memory\n",
        "        batch_size = min(50, len(file_paths))  # Process 50 files at a time\n",
        "        all_bags = []\n",
        "\n",
        "        for i in range(0, len(file_paths), batch_size):\n",
        "            batch_files = file_paths[i:i+batch_size]\n",
        "            logger.info(f\"üì¶ Processing batch {i//batch_size + 1}/{(len(file_paths)-1)//batch_size + 1}\")\n",
        "\n",
        "            batch_bags = [process_log_file(path) for path in batch_files]\n",
        "            all_bags.extend(batch_bags)\n",
        "\n",
        "        # Combine all bags\n",
        "        logger.info(\"üîÑ Combining all parsed data...\")\n",
        "        combined_bag = db.concat(all_bags)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        columns = ['ip', 'timestamp', 'method', 'url', 'protocol', 'status', 'bytes', 'referer', 'user_agent']\n",
        "        combined_ddf = combined_bag.to_dataframe(columns=columns)\n",
        "\n",
        "        # Compute to pandas (with memory monitoring)\n",
        "        logger.info(\"üíæ Converting to pandas DataFrame...\")\n",
        "        combined_df = combined_ddf.compute()\n",
        "\n",
        "        processing_time = datetime.now() - start_time\n",
        "        logger.info(f\"‚úÖ Successfully parsed {len(combined_df)} log entries in {processing_time}\")\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in parse_all_logs: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Parse all log files (this may take several minutes)\n",
        "print(\"üöÄ Starting log parsing process...\")\n",
        "combined_df = parse_all_logs(file_paths)\n",
        "\n",
        "if not combined_df.empty:\n",
        "    print(f\"‚úÖ Successfully parsed {len(combined_df)} log entries\")\n",
        "    # Save raw parsed data\n",
        "    combined_df.to_parquet('/content/drive/My Drive/parsed_logs.parquet', index=False)\n",
        "    print(\"üíæ Raw data saved to parsed_logs.parquet\")\n",
        "\n",
        "    # Show sample data\n",
        "    print(\"\\nüìä Sample of parsed data:\")\n",
        "    print(combined_df.head())\n",
        "else:\n",
        "    print(\"‚ùå Failed to parse logs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fusFtOovxnnU",
        "outputId": "fdba333c-de2f-4edd-ea13-5098bfa21c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting log parsing process...\n",
            "‚úÖ Successfully parsed 4296797 log entries\n",
            "üíæ Raw data saved to parsed_logs.parquet\n",
            "\n",
            "üìä Sample of parsed data:\n",
            "               ip                   timestamp method  \\\n",
            "0  34.206.152.150  03/Jul/2022:02:32:54 -0700   HEAD   \n",
            "1  34.206.152.150  03/Jul/2022:02:32:54 -0700   HEAD   \n",
            "2    85.208.98.17  03/Jul/2022:02:39:07 -0700    GET   \n",
            "0    85.208.98.17  03/Jul/2022:02:39:10 -0700    GET   \n",
            "1   51.222.253.17  03/Jul/2022:02:39:28 -0700    GET   \n",
            "\n",
            "                                               url  protocol status  bytes  \\\n",
            "0                                                /  HTTP/1.1    301    186   \n",
            "1                                                /  HTTP/1.1    200    359   \n",
            "2                                                /  HTTP/1.1    301    419   \n",
            "0                                                /  HTTP/1.1    200  13142   \n",
            "1  /Datasets%20Description/HTML_Bro_log_2/?C=M;O=D  HTTP/1.1    200    788   \n",
            "\n",
            "                referer                                         user_agent  \n",
            "0                     -                          DreamHost SiteMonitor 1.0  \n",
            "1  http://66.33.221.139                          DreamHost SiteMonitor 1.0  \n",
            "2                     -                                         SEMrushBot  \n",
            "0   http://secrepo.com/                                         SEMrushBot  \n",
            "1                     -  Mozilla/5.0 (compatible; AhrefsBot/7.0; +http:...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_vulnerability_features(df):\n",
        "    \"\"\"Extract features specifically for vulnerability detection\"\"\"\n",
        "    try:\n",
        "        logger.info(\"üîç Extracting vulnerability-focused features...\")\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Basic cleaning\n",
        "        df = df.copy()\n",
        "        df['status'] = pd.to_numeric(df['status'], errors='coerce').fillna(0).astype(int)\n",
        "        df['bytes'] = pd.to_numeric(df['bytes'].replace('-', '0'), errors='coerce').fillna(0)\n",
        "\n",
        "        # Parse timestamp with better error handling\n",
        "        logger.info(\"üìÖ Parsing timestamps...\")\n",
        "\n",
        "        # Try multiple timestamp formats\n",
        "        timestamp_formats = [\n",
        "            '%d/%b/%Y:%H:%M:%S %z',  # Standard Apache format with timezone\n",
        "            '%d/%b/%Y:%H:%M:%S',     # Without timezone\n",
        "            '%Y-%m-%d %H:%M:%S',     # Standard format\n",
        "            '%Y/%m/%d %H:%M:%S'      # Alternative format\n",
        "        ]\n",
        "\n",
        "        df['timestamp'] = None\n",
        "        for fmt in timestamp_formats:\n",
        "            if df['timestamp'].isna().all():\n",
        "                try:\n",
        "                    df['timestamp'] = pd.to_datetime(df['timestamp'], format=fmt, errors='coerce')\n",
        "                    if not df['timestamp'].isna().all():\n",
        "                        logger.info(f\"‚úÖ Successfully parsed timestamps with format: {fmt}\")\n",
        "                        break\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        # If timestamp parsing still fails, create a dummy timestamp\n",
        "        if df['timestamp'].isna().all():\n",
        "            logger.warning(\"‚ö†Ô∏è Timestamp parsing failed, using sequential timestamps\")\n",
        "            df['timestamp'] = pd.date_range(start='2024-01-01', periods=len(df), freq='1min')\n",
        "\n",
        "        # Vulnerability-specific features\n",
        "        logger.info(\"üéØ Creating vulnerability indicators...\")\n",
        "\n",
        "        # 1. CMS and Framework Probing\n",
        "        cms_patterns = [\n",
        "            'wp-login', 'wp-admin', 'wp-content', 'wp-includes',  # WordPress\n",
        "            'joomla', 'administrator', 'com_',  # Joomla\n",
        "            'drupal', 'node/', 'admin/',  # Drupal\n",
        "            'magento', 'app/etc', 'downloader',  # Magento\n",
        "            'udd.php', 'shell.php', 'c99.php',  # Common shells\n",
        "        ]\n",
        "        df['is_cms_probe'] = df['url'].str.contains('|'.join(cms_patterns), case=False, na=False)\n",
        "\n",
        "        # 2. Sensitive File Access\n",
        "        sensitive_patterns = [\n",
        "            r'\\.gz$', r'\\.zip$', r'\\.tar$', r'\\.log$', r'\\.sql$',  # Archives & logs\n",
        "            r'\\.env$', r'\\.config$', r'\\.ini$', r'\\.conf$',  # Config files\n",
        "            r'backup', r'dump', r'export',  # Backup files\n",
        "            r'passwd', r'shadow', r'htpasswd',  # System files\n",
        "        ]\n",
        "        df['is_sensitive_file'] = df['url'].str.contains('|'.join(sensitive_patterns), case=False, na=False)\n",
        "\n",
        "        # 3. Attack Patterns\n",
        "        attack_patterns = [\n",
        "            r'union.*select', r'script.*alert', r'javascript:',  # SQLi, XSS\n",
        "            r'\\.\\./\\.\\./\\.\\.',  # Directory traversal\n",
        "            r'<script', r'eval\\(', r'exec\\(',  # Code injection\n",
        "            r'cmd=', r'exec=', r'shell=',  # Command injection\n",
        "        ]\n",
        "        df['is_attack_pattern'] = df['url'].str.contains('|'.join(attack_patterns), case=False, na=False)\n",
        "\n",
        "        # 4. Error Patterns (Reconnaissance)\n",
        "        df['is_404'] = (df['status'] == 404)\n",
        "        df['is_403'] = (df['status'] == 403)\n",
        "        df['is_500'] = (df['status'] == 500)\n",
        "        df['is_error'] = df['status'].isin([400, 401, 403, 404, 500, 502, 503])\n",
        "\n",
        "        # 5. Bot Detection\n",
        "        bot_patterns = [\n",
        "            'bot', 'crawler', 'spider', 'scraper',\n",
        "            'curl', 'wget', 'python', 'php',\n",
        "            'scanner', 'nikto', 'sqlmap'\n",
        "        ]\n",
        "\n",
        "        # Handle missing user_agent column\n",
        "        if 'user_agent' not in df.columns:\n",
        "            logger.warning(\"‚ö†Ô∏è user_agent column not found, creating dummy column\")\n",
        "            df['user_agent'] = ''\n",
        "\n",
        "        df['is_bot'] = df['user_agent'].str.contains('|'.join(bot_patterns), case=False, na=False)\n",
        "\n",
        "        # 6. Suspicious User Agents\n",
        "        df['is_empty_ua'] = (df['user_agent'] == '') | (df['user_agent'] == '-') | df['user_agent'].isna()\n",
        "        df['is_old_browser'] = df['user_agent'].str.contains('MSIE [1-6]|Windows 95|Windows 98', case=False, na=False)\n",
        "\n",
        "        logger.info(\"üìä Aggregating features by IP address...\")\n",
        "\n",
        "        # IP-based aggregations for vulnerability assessment\n",
        "        agg_dict = {\n",
        "            'url': ['count', 'nunique'],\n",
        "            'status': ['mean', 'std'],\n",
        "            'bytes': ['sum', 'mean'],\n",
        "            'is_404': 'sum',\n",
        "            'is_403': 'sum',\n",
        "            'is_500': 'sum',\n",
        "            'is_error': 'sum',\n",
        "            'is_cms_probe': 'sum',\n",
        "            'is_sensitive_file': 'sum',\n",
        "            'is_attack_pattern': 'sum',\n",
        "            'is_bot': 'any',\n",
        "            'is_empty_ua': 'any',\n",
        "            'is_old_browser': 'any',\n",
        "            'timestamp': ['min', 'max', 'count']\n",
        "        }\n",
        "\n",
        "        # Add method aggregation if column exists\n",
        "        if 'method' in df.columns:\n",
        "            agg_dict['method'] = lambda x: x.value_counts().index[0] if len(x) > 0 else 'GET'\n",
        "\n",
        "        ip_features = df.groupby('ip').agg(agg_dict).reset_index()\n",
        "\n",
        "        # Flatten column names\n",
        "        new_columns = ['ip']\n",
        "        for col in ip_features.columns[1:]:\n",
        "            if isinstance(col, tuple):\n",
        "                if col[1] == '<lambda>':\n",
        "                    new_columns.append(f\"common_{col[0]}\")\n",
        "                else:\n",
        "                    new_columns.append(f\"{col[0]}_{col[1]}\")\n",
        "            else:\n",
        "                new_columns.append(col)\n",
        "\n",
        "        ip_features.columns = new_columns\n",
        "\n",
        "        # Ensure we have the expected columns (with fallbacks)\n",
        "        expected_cols = {\n",
        "            'request_count': 'url_count',\n",
        "            'unique_urls': 'url_nunique',\n",
        "            'avg_status': 'status_mean',\n",
        "            'status_std': 'status_std',\n",
        "            'total_bytes': 'bytes_sum',\n",
        "            'avg_bytes': 'bytes_mean',\n",
        "            'count_404': 'is_404_sum',\n",
        "            'count_403': 'is_403_sum',\n",
        "            'count_500': 'is_500_sum',\n",
        "            'count_errors': 'is_error_sum',\n",
        "            'cms_probes': 'is_cms_probe_sum',\n",
        "            'sensitive_files': 'is_sensitive_file_sum',\n",
        "            'attack_patterns': 'is_attack_pattern_sum',\n",
        "            'is_bot': 'is_bot_any',\n",
        "            'empty_user_agent': 'is_empty_ua_any',\n",
        "            'old_browser': 'is_old_browser_any',\n",
        "            'first_seen': 'timestamp_min',\n",
        "            'last_seen': 'timestamp_max',\n",
        "            'total_requests': 'timestamp_count'\n",
        "        }\n",
        "\n",
        "        # Rename columns to expected names\n",
        "        rename_dict = {}\n",
        "        for expected, actual in expected_cols.items():\n",
        "            if actual in ip_features.columns:\n",
        "                rename_dict[actual] = expected\n",
        "\n",
        "        ip_features = ip_features.rename(columns=rename_dict)\n",
        "\n",
        "        # Fill missing columns with defaults\n",
        "        for expected_col in expected_cols.keys():\n",
        "            if expected_col not in ip_features.columns:\n",
        "                if expected_col in ['is_bot', 'empty_user_agent', 'old_browser']:\n",
        "                    ip_features[expected_col] = False\n",
        "                elif 'count' in expected_col or 'sum' in expected_col:\n",
        "                    ip_features[expected_col] = 0\n",
        "                elif 'rate' in expected_col:\n",
        "                    ip_features[expected_col] = 0.0\n",
        "                else:\n",
        "                    ip_features[expected_col] = 0\n",
        "\n",
        "        # Calculate derived features\n",
        "        ip_features['error_rate'] = ip_features['count_errors'] / ip_features['request_count'].replace(0, 1)\n",
        "        ip_features['404_rate'] = ip_features['count_404'] / ip_features['request_count'].replace(0, 1)\n",
        "        ip_features['diversity_score'] = ip_features['unique_urls'] / ip_features['request_count'].replace(0, 1)\n",
        "\n",
        "        # Time-based features (with error handling)\n",
        "        try:\n",
        "            if 'first_seen' in ip_features.columns and 'last_seen' in ip_features.columns:\n",
        "                # Ensure timestamps are datetime\n",
        "                ip_features['first_seen'] = pd.to_datetime(ip_features['first_seen'])\n",
        "                ip_features['last_seen'] = pd.to_datetime(ip_features['last_seen'])\n",
        "\n",
        "                duration_seconds = (ip_features['last_seen'] - ip_features['first_seen']).dt.total_seconds()\n",
        "                ip_features['session_duration'] = duration_seconds / 3600  # hours\n",
        "                ip_features['requests_per_hour'] = ip_features['request_count'] / (ip_features['session_duration'] + 0.01)\n",
        "            else:\n",
        "                ip_features['session_duration'] = 1.0  # Default 1 hour\n",
        "                ip_features['requests_per_hour'] = ip_features['request_count']\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"‚ö†Ô∏è Time-based feature calculation failed: {e}, using defaults\")\n",
        "            ip_features['session_duration'] = 1.0\n",
        "            ip_features['requests_per_hour'] = ip_features['request_count']\n",
        "\n",
        "        # Vulnerability scores\n",
        "        ip_features['vulnerability_score'] = (\n",
        "            ip_features['cms_probes'] * 3 +\n",
        "            ip_features['sensitive_files'] * 2 +\n",
        "            ip_features['attack_patterns'] * 5 +\n",
        "            ip_features['404_rate'] * ip_features['request_count'] * 0.1\n",
        "        )\n",
        "\n",
        "        processing_time = datetime.now() - start_time\n",
        "        logger.info(f\"‚úÖ Feature extraction completed in {processing_time}\")\n",
        "        logger.info(f\"üìà Generated features for {len(ip_features)} unique IP addresses\")\n",
        "\n",
        "        # Log feature statistics\n",
        "        vulnerable_ips = ip_features[ip_features['vulnerability_score'] > 0]\n",
        "        logger.info(f\"üö® Found {len(vulnerable_ips)} IPs with vulnerability indicators\")\n",
        "        logger.info(f\"ü§ñ Bot traffic detected from {ip_features['is_bot'].sum()} IPs\")\n",
        "        logger.info(f\"üîç CMS probing detected: {ip_features['cms_probes'].sum()} attempts\")\n",
        "        logger.info(f\"üìÅ Sensitive file access: {ip_features['sensitive_files'].sum()} attempts\")\n",
        "\n",
        "        return ip_features\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in feature extraction: {e}\")\n",
        "        import traceback\n",
        "        logger.error(f\"Full traceback: {traceback.format_exc()}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Extract vulnerability features\n",
        "print(\"üîç Extracting vulnerability features...\")\n",
        "features_df = extract_vulnerability_features(combined_df)\n",
        "\n",
        "if not features_df.empty:\n",
        "    print(f\"‚úÖ Features extracted for {len(features_df)} unique IPs\")\n",
        "    # Save features\n",
        "    features_df.to_parquet('/content/drive/My Drive/vulnerability_features.parquet', index=False)\n",
        "    print(\"üíæ Features saved to vulnerability_features.parquet\")\n",
        "\n",
        "    # Show sample features\n",
        "    print(\"\\nüìä Sample features:\")\n",
        "    display_cols = ['ip', 'request_count', 'vulnerability_score', 'cms_probes', 'attack_patterns']\n",
        "    available_cols = [col for col in display_cols if col in features_df.columns]\n",
        "    print(features_df[available_cols].head())\n",
        "else:\n",
        "    print(\"‚ùå Failed to extract features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV1x3u4IIoZ0",
        "outputId": "e8f8309d-ec66-4a6d-b50e-e1cfdbdb02e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Extracting vulnerability features...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:‚ö†Ô∏è Timestamp parsing failed, using sequential timestamps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Features extracted for 259629 unique IPs\n",
            "üíæ Features saved to vulnerability_features.parquet\n",
            "\n",
            "üìä Sample features:\n",
            "            ip  request_count  vulnerability_score  cms_probes  \\\n",
            "0   1.0.138.21              1                  0.0           0   \n",
            "1   1.0.144.68              1                  0.0           0   \n",
            "2  1.0.197.183              2                  3.0           1   \n",
            "3  1.0.200.186              2                  3.0           1   \n",
            "4   1.0.201.89              2                  6.2           2   \n",
            "\n",
            "   attack_patterns  \n",
            "0                0  \n",
            "1                0  \n",
            "2                0  \n",
            "3                0  \n",
            "4                0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_vulnerabilities(features_df):\n",
        "    \"\"\"Perform detailed vulnerability analysis\"\"\"\n",
        "    try:\n",
        "        logger.info(\"üî¨ Performing detailed vulnerability analysis...\")\n",
        "\n",
        "        # Top vulnerable IPs\n",
        "        top_vulnerable = features_df.nlargest(10, 'vulnerability_score')[['ip', 'vulnerability_score', 'request_count', 'cms_probes', 'attack_patterns']]\n",
        "        logger.info(\"üö® Top 10 most suspicious IPs:\")\n",
        "        for _, row in top_vulnerable.iterrows():\n",
        "            logger.info(f\"   {row['ip']}: Score={row['vulnerability_score']:.2f}, Requests={row['request_count']}, CMS={row['cms_probes']}, Attacks={row['attack_patterns']}\")\n",
        "\n",
        "        # Attack summary\n",
        "        total_cms_probes = features_df['cms_probes'].sum()\n",
        "        total_sensitive_files = features_df['sensitive_files'].sum()\n",
        "        total_attack_patterns = features_df['attack_patterns'].sum()\n",
        "\n",
        "        logger.info(\"üìà Attack Summary:\")\n",
        "        logger.info(f\"   üéØ CMS Probing Attempts: {total_cms_probes}\")\n",
        "        logger.info(f\"   üìÅ Sensitive File Access: {total_sensitive_files}\")\n",
        "        logger.info(f\"   ‚öîÔ∏è Attack Patterns Detected: {total_attack_patterns}\")\n",
        "\n",
        "        # Bot analysis\n",
        "        bot_ips = features_df[features_df['is_bot'] == True]\n",
        "        logger.info(f\"ü§ñ Bot Activity: {len(bot_ips)} IPs identified as bots\")\n",
        "\n",
        "        # Error analysis\n",
        "        high_error_ips = features_df[features_df['error_rate'] > 0.5]\n",
        "        logger.info(f\"‚ùó High Error Rate: {len(high_error_ips)} IPs with >50% error rate\")\n",
        "\n",
        "        # Save detailed analysis\n",
        "        analysis_results = {\n",
        "            'top_vulnerable_ips': top_vulnerable,\n",
        "            'attack_summary': {\n",
        "                'cms_probes': int(total_cms_probes),\n",
        "                'sensitive_files': int(total_sensitive_files),\n",
        "                'attack_patterns': int(total_attack_patterns)\n",
        "            },\n",
        "            'bot_ips': len(bot_ips),\n",
        "            'high_error_ips': len(high_error_ips)\n",
        "        }\n",
        "\n",
        "        return analysis_results\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in vulnerability analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "# Perform vulnerability analysis\n",
        "print(\"üî¨ Analyzing vulnerabilities...\")\n",
        "analysis_results = analyze_vulnerabilities(features_df)\n",
        "\n",
        "if analysis_results:\n",
        "    print(\"‚úÖ Vulnerability analysis completed\")\n",
        "    print(f\"üìä Top vulnerable IPs identified: {len(analysis_results['top_vulnerable_ips'])}\")\n",
        "    print(f\"üéØ Total attack patterns: {analysis_results['attack_summary']['attack_patterns']}\")\n",
        "else:\n",
        "    print(\"‚ùå Vulnerability analysis failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZHtN99TIo7b",
        "outputId": "03de3ef0-27cc-4e8c-f731-cb83b852b7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Analyzing vulnerabilities...\n",
            "‚úÖ Vulnerability analysis completed\n",
            "üìä Top vulnerable IPs identified: 10\n",
            "üéØ Total attack patterns: 1782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vulnerability_models(features_df):\n",
        "    \"\"\"Train multiple models for vulnerability detection\"\"\"\n",
        "    try:\n",
        "        logger.info(\"ü§ñ Training vulnerability detection models...\")\n",
        "        start_time = datetime.now()\n",
        "\n",
        "        # Prepare features for modeling\n",
        "        feature_columns = [\n",
        "            'request_count', 'unique_urls', 'avg_status', 'status_std',\n",
        "            'total_bytes', 'avg_bytes', 'count_404', 'count_403', 'count_500',\n",
        "            'count_errors', 'cms_probes', 'sensitive_files', 'attack_patterns',\n",
        "            'error_rate', '404_rate', 'diversity_score', 'session_duration',\n",
        "            'requests_per_hour', 'vulnerability_score'\n",
        "        ]\n",
        "\n",
        "        X = features_df[feature_columns].fillna(0)\n",
        "\n",
        "        # Create labels using multiple heuristics\n",
        "        labels = (\n",
        "            (features_df['cms_probes'] > 0) |\n",
        "            (features_df['attack_patterns'] > 0) |\n",
        "            (features_df['vulnerability_score'] > 5) |\n",
        "            ((features_df['404_rate'] > 0.3) & (features_df['request_count'] > 10))\n",
        "        ).astype(int)\n",
        "\n",
        "        logger.info(f\"üè∑Ô∏è Label distribution - Malicious: {labels.sum()}, Benign: {len(labels) - labels.sum()}\")\n",
        "\n",
        "        if labels.sum() == 0:\n",
        "            logger.warning(\"‚ö†Ô∏è No malicious IPs detected with current heuristics\")\n",
        "            return None\n",
        "\n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_scaled, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "        # Train Random Forest\n",
        "        logger.info(\"üå≤ Training Random Forest classifier...\")\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate Random Forest\n",
        "        rf_pred = rf_model.predict(X_test)\n",
        "        logger.info(\"üìä Random Forest Results:\")\n",
        "        logger.info(f\"\\n{classification_report(y_test, rf_pred)}\")\n",
        "\n",
        "        # Train Isolation Forest for anomaly detection\n",
        "        logger.info(\"üîç Training Isolation Forest for anomaly detection...\")\n",
        "        iso_model = IsolationForest(contamination=0.1, random_state=42)\n",
        "        iso_model.fit(X_train[y_train == 0])  # Train only on benign data\n",
        "\n",
        "        # Feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': feature_columns,\n",
        "            'importance': rf_model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        logger.info(\"üéØ Top 5 most important features:\")\n",
        "        for _, row in feature_importance.head().iterrows():\n",
        "            logger.info(f\"   {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "        # Save models\n",
        "        models = {\n",
        "            'random_forest': rf_model,\n",
        "            'isolation_forest': iso_model,\n",
        "            'scaler': scaler,\n",
        "            'feature_columns': feature_columns,\n",
        "            'feature_importance': feature_importance\n",
        "        }\n",
        "\n",
        "        joblib.dump(models, '/content/drive/My Drive/vulnerability_models.pkl')\n",
        "        logger.info(\"üíæ Models saved to /content/drive/My Drive/vulnerability_models.pkl\")\n",
        "\n",
        "        processing_time = datetime.now() - start_time\n",
        "        logger.info(f\"‚úÖ Model training completed in {processing_time}\")\n",
        "\n",
        "        return models\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"‚ùå Error in model training: {e}\")\n",
        "        return None\n",
        "\n",
        "# Train machine learning models\n",
        "print(\"ü§ñ Training vulnerability detection models...\")\n",
        "models = train_vulnerability_models(features_df)\n",
        "\n",
        "if models:\n",
        "    print(\"‚úÖ Models trained successfully\")\n",
        "    print(\"üìä Feature importance:\")\n",
        "    print(models['feature_importance'].head())\n",
        "else:\n",
        "    print(\"‚ùå Model training failed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1T_ryPk37vT",
        "outputId": "62c23474-6402-45a8-f19d-9e2794f93797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§ñ Training vulnerability detection models...\n",
            "‚úÖ Models trained successfully\n",
            "üìä Feature importance:\n",
            "                feature  importance\n",
            "18  vulnerability_score    0.395290\n",
            "10           cms_probes    0.132072\n",
            "2            avg_status    0.128353\n",
            "11      sensitive_files    0.102298\n",
            "6             count_404    0.048026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate final summary\n",
        "print(\"=\"*60)\n",
        "print(\"üéâ VULNERABILITY DETECTION PIPELINE COMPLETED!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if not combined_df.empty:\n",
        "    print(f\"üìä Processed {len(combined_df)} log entries from {len(file_paths)} files\")\n",
        "\n",
        "if not features_df.empty:\n",
        "    print(f\"üéØ Analyzed {len(features_df)} unique IP addresses\")\n",
        "\n",
        "    # Summary statistics\n",
        "    vulnerable_ips = features_df[features_df['vulnerability_score'] > 0]\n",
        "    print(f\"üö® Vulnerable IPs detected: {len(vulnerable_ips)}\")\n",
        "    print(f\"ü§ñ Bot IPs detected: {features_df['is_bot'].sum()}\")\n",
        "    print(f\"üîç CMS probe attempts: {features_df['cms_probes'].sum()}\")\n",
        "    print(f\"‚öîÔ∏è Attack patterns: {features_df['attack_patterns'].sum()}\")\n",
        "\n",
        "print(\"\\nüìÅ Output files saved to Google Drive:\")\n",
        "print(\"   - parsed_logs.parquet (raw parsed data)\")\n",
        "print(\"   - vulnerability_features.parquet (extracted features)\")\n",
        "if models:\n",
        "    print(\"   - vulnerability_models.pkl (trained ML models)\")\n",
        "print(\"   - vulnerability_detection.log (execution log)\")\n",
        "\n",
        "print(\"\\nüí° Next steps:\")\n",
        "print(\"   1. Review the vulnerability analysis results\")\n",
        "print(\"   2. Investigate suspicious IP addresses\")\n",
        "print(\"   3. Use trained models for real-time detection\")\n",
        "print(\"   4. Implement security measures based on findings\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhEV6aWO38gv",
        "outputId": "86f8f447-1319-4d23-98ea-e4bd10ddac6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üéâ VULNERABILITY DETECTION PIPELINE COMPLETED!\n",
            "============================================================\n",
            "üìä Processed 4296797 log entries from 2741 files\n",
            "üéØ Analyzed 259629 unique IP addresses\n",
            "üö® Vulnerable IPs detected: 149207\n",
            "ü§ñ Bot IPs detected: 37228\n",
            "üîç CMS probe attempts: 187227\n",
            "‚öîÔ∏è Attack patterns: 1782\n",
            "\n",
            "üìÅ Output files saved to Google Drive:\n",
            "   - parsed_logs.parquet (raw parsed data)\n",
            "   - vulnerability_features.parquet (extracted features)\n",
            "   - vulnerability_models.pkl (trained ML models)\n",
            "   - vulnerability_detection.log (execution log)\n",
            "\n",
            "üí° Next steps:\n",
            "   1. Review the vulnerability analysis results\n",
            "   2. Investigate suspicious IP addresses\n",
            "   3. Use trained models for real-time detection\n",
            "   4. Implement security measures based on findings\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}